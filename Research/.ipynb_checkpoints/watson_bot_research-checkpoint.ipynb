{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "Auth_key = \"rEi)4Xt7RnJf4tqP)pk5nQ((\"\n",
    "from html import unescape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type a tag from the above list => python\n"
     ]
    }
   ],
   "source": [
    "tag_to_search = input(\"Type a tag from the above list => \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_more =1\n",
    "no_of_qs = str(10)\n",
    "ques_titles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 0  #kept in separate cell. if http or throttle error occurs we can change this value till it is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File No  0\n",
      "python0.txt is written \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "no_of_doc = 1\n",
    "for i in range(no_of_doc):\n",
    "    page = str(int(page)+1)\n",
    "    url=\"https://api.stackexchange.com//2.2/tags/\"+tag_to_search+\"/faq?key=\"+Auth_key+\"&page=\"+page+\"&pagesize=\"+no_of_qs+\"&site=stackoverflow\"\n",
    "    r = requests.get(url)\n",
    "    ques = r.json()\n",
    "    print(\"File No \",i)\n",
    "#     text_file = open(tag_to_search+str(i)+\".txt\", \"a\",encoding='utf-8')\n",
    "    for j in range(len(ques['items'])):\n",
    "        qstn = unescape(ques['items'][j]['title'])\n",
    "        if qstn[-1] != \"?\":\n",
    "            qstn_eta = qstn[:-2] + \".\"\n",
    "        else:\n",
    "            qstn_eta = qstn\n",
    "#         print(qstn_eta)\n",
    "        ques_titles.append(qstn)\n",
    "#         text_file.write(qstn_eta)\n",
    "#         text_file.write(\"\\n\")\n",
    "#     text_file.close()\n",
    "    print(tag_to_search+str(i)+\".txt is written\",\"\\n\")\n",
    "    if i != (no_of_doc-1):\n",
    "        print(\"Stackoverflow throttle limit.Let's wait for 2 seconds\")\n",
    "        time.sleep(2)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from collections import Counter\n",
    "# import json\n",
    "\n",
    "# # def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "# # WORDS = Counter(words(open('this_file.txt').read()))\n",
    "\n",
    "# with open('SO_tags_dict.txt') as json_file:  \n",
    "#     WORDS = json.load(json_file)\n",
    "\n",
    "# def P(word, N=sum(WORDS.values())): \n",
    "#     \"Probability of `word`.\"\n",
    "#     return WORDS[word] / N\n",
    "\n",
    "# def correction(word): \n",
    "#     \"Most probable spelling correction for word.\"\n",
    "#     return max(candidates(word), key=P)\n",
    "\n",
    "# def candidates(word): \n",
    "#     \"Generate possible spelling corrections for word.\"\n",
    "# #     print(known([word]),known(edits1(word)),known(edits2(word)),word)\n",
    "#     return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "# def known(words): \n",
    "#     \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "#     return set(w for w in words if w in WORDS)\n",
    "\n",
    "# def edits1(word):\n",
    "#     \"All edits that are one edit away from `word`.\"\n",
    "#     letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "#     splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "#     deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "#     transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "#     replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "#     inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "#     return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "\n",
    "# def edits2(word): \n",
    "#     \"All edits that are two edits away from `word`.\"\n",
    "#     return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_stopwords(words_list_eta):\n",
    "#     from nltk.corpus import stopwords\n",
    "#     import nltk\n",
    "# #     nltk.download('stopwords')\n",
    "\n",
    "#     stop_words = stopwords.words('english')\n",
    "\n",
    "#     return [word for word in words_list_eta if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def autocorrection(words_list_2_eta):\n",
    "#     words_list_3 = []\n",
    "#     for i in words_list_2_eta:\n",
    "#         words_list_3.append(correction(i))\n",
    "#     return words_list_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def watson_nlu(text_test_eta):\n",
    "#     import json\n",
    "#     from ibm_watson import NaturalLanguageUnderstandingV1\n",
    "#     from ibm_watson.natural_language_understanding_v1 import Features,KeywordsOptions ,SyntaxOptions, SyntaxOptionsTokens,SentimentOptions,EntitiesOptions\n",
    "\n",
    "#     credentials = {\"api_key\": \"wEHseKE5Iqv2PfZwtOP6FVQYOeoJxIa-32Xu5QbNkBRM\",\n",
    "#                    \"api_url\": \"https://gateway.watsonplatform.net/natural-language-understanding/api\"}\n",
    "\n",
    "\n",
    "#     natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "#         version='2018-11-16',\n",
    "#         iam_apikey=credentials['api_key'],\n",
    "#         url=credentials['api_url']\n",
    "#     )\n",
    "#     response_eta = natural_language_understanding.analyze(model = \"169529e7-6cc2-48ee-b825-191e15ca0782\",\n",
    "#         text=text_test_eta,\n",
    "#         features=Features(keywords=KeywordsOptions(),syntax=SyntaxOptions(\n",
    "#             sentences=True,\n",
    "#             tokens=SyntaxOptionsTokens(\n",
    "#               lemma=True,\n",
    "#               part_of_speech=True,\n",
    "#             )),sentiment=SentimentOptions(),entities=EntitiesOptions(sentiment=True,limit=1,model=\"169529e7-6cc2-48ee-b825-191e15ca0782\"))).get_result()\n",
    "# #     print(response_eta['usage'])\n",
    "#     return response_eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is done as nlu returns group of objects sometimes\n",
    "\n",
    "# def relevance_list_fun(response_eta):\n",
    "#     relevance_list = [x['text'] for x in response_eta['keywords']]\n",
    "#     relevance_list = [x.split() for x in relevance_list]\n",
    "#     final_list_eta = []\n",
    "#     for i in relevance_list:\n",
    "#     #     print(i,\"len is \",len(i))\n",
    "#         if len(i) > 1:\n",
    "#     #         print(i)\n",
    "#             for j in i:\n",
    "#                 final_list_eta.append(j)\n",
    "#         else:\n",
    "#             final_list_eta.append(i[0])\n",
    "#     return final_list_eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stemming(string_eta):\n",
    "#     from nltk.stem import PorterStemmer\n",
    "#     from nltk.tokenize import word_tokenize \n",
    "\n",
    "#     ps = PorterStemmer()\n",
    "\n",
    "#     words = word_tokenize(string_eta)\n",
    "\n",
    "#     for w in words:\n",
    "#         print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ques_titles = [\"where to find all the keys of a dictionary in python\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=-1\n",
    "# for user_input in ques_titles:\n",
    "#     i = i+1\n",
    "#     nlu_response = watson_nlu(user_input)\n",
    "#     stemming(ques_titles[i])\n",
    "#     tags_list = [x['text'] for x in nlu_response['entities']]\n",
    "    \n",
    "#     print(tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "#moh bot\n",
    "\n",
    "credentials_moh = {\"api_key\": \"HTHsjdKJzDGeQlHu_uJijwKbb48__vZbypti5MVchZL7\",\n",
    "                   \"api_url\": \"https://gateway.watsonplatform.net/assistant/api\",\n",
    "               \"workspace_id\":\"f2d82aa9-b6a3-4657-a6eb-01ff1cfec5a8\"\n",
    "              }\n",
    "\n",
    "# shiv bot\n",
    "credentials_shiv = {\"api_key\": \"BNfXNhGUERnE1hpQCXrTTvLNqJhrxsiQ2mHKs0qhmecV\",\n",
    "                   \"api_url\": \"https://gateway-lon.watsonplatform.net/assistant/api\",\n",
    "               \"workspace_id\":\"2fe0180a-0490-45b9-bf2a-4dd4b9e31939\"\n",
    "              }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query is : How to test multiple variables against a value?\n",
      "CHATBOT_shiv TAGS ARE : ['testing:0.7', 'multiplicity:0.7', 'multiplication:0.7', 'variables:1', 'against:1']\n",
      "CHATBOT_moh TAGS ARE : ['testing:1', 'multiplication:0.7', 'multiplicity:0.7', 'variables:1', 'against:1']\n",
      "\n",
      "\n",
      "Query is : Understanding slice notation\n",
      "CHATBOT_shiv TAGS ARE : ['slice:1', 'notation:1']\n",
      "CHATBOT_moh TAGS ARE : ['slice:1', 'notation:1']\n",
      "\n",
      "\n",
      "Query is : \"Least Astonishment\" and the Mutable Default Argument\n",
      "CHATBOT_shiv TAGS ARE : ['least-astonishment:0.66', 'mutable:1', 'default:1', 'arguments:0.7']\n",
      "CHATBOT_moh TAGS ARE : ['least-astonishment:0.66', 'mutable:1', 'default:1', 'arguments:1']\n",
      "\n",
      "\n",
      "Query is : Asking the user for input until they give a valid response\n",
      "CHATBOT_shiv TAGS ARE : ['input:1', 'validating:0.7', 'validation:0.7', 'response:1']\n",
      "CHATBOT_moh TAGS ARE : ['alexa-skills-kit:0.7', 'user:1', 'for-loop:1', 'input:1', 'validation:0.7', 'response:1']\n",
      "\n",
      "\n",
      "Query is : How to clone or copy a list?\n",
      "CHATBOT_shiv TAGS ARE : ['clone:1', 'copy:1', 'list:1']\n",
      "CHATBOT_moh TAGS ARE : ['clone:1', 'copy:1', 'list:1']\n",
      "\n",
      "\n",
      "Query is : How do I create a variable number of variables?\n",
      "CHATBOT_shiv TAGS ARE : ['doi:0.8', 'variables:0.7', 'numbered:0.7', 'numbers:0.7', 'variables:1']\n",
      "CHATBOT_moh TAGS ARE : ['doi:0.8', 'variables:1', 'numbers:1', 'variables:1']\n",
      "\n",
      "\n",
      "Query is : List of lists changes reflected across sublists unexpectedly\n",
      "CHATBOT_shiv TAGS ARE : ['list:1', 'listings:0.7', 'list:0.7', 'reflection:0.7', 'reflect:0.7', 'sublist:0.88']\n",
      "CHATBOT_moh TAGS ARE : ['list:1', 'list:1', 'reflection:0.7', 'reflect:0.7', 'sublist:0.88']\n",
      "\n",
      "\n",
      "Query is : How to make good reproducible pandas examples\n",
      "CHATBOT_shiv TAGS ARE : ['reproducible-research:0.57', 'pandas:1']\n",
      "CHATBOT_moh TAGS ARE : ['makefile:1', 'reproducible-research:0.57', 'pandas:1']\n",
      "\n",
      "\n",
      "Query is : How to remove items from a list while iterating?\n",
      "CHATBOT_shiv TAGS ARE : ['removable:0.7', 'items:1', 'list:1', 'iteration:0.7', 'iterate:0.7', 'iterator:0.7', 'iterable:0.7']\n",
      "CHATBOT_moh TAGS ARE : ['removable:0.7', 'items:1', 'list:1', 'while-loop:1', 'iterator:0.7', 'iteration:0.7', 'iterable:0.7', 'loops:0.7']\n",
      "\n",
      "\n",
      "Query is : How to make a flat list out of list of lists\n",
      "CHATBOT_shiv TAGS ARE : ['flat:1', 'list:1', 'out:1', 'list:1', 'listings:0.7', 'list:0.7']\n",
      "CHATBOT_moh TAGS ARE : ['makefile:1', 'flat:1', 'list:1', 'out:1', 'list:1', 'list:1']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ibm_watson\n",
    "\n",
    "for user_input in ques_titles:\n",
    "#     user_input = str(input('enter text :'))\n",
    "    print(\"Query is :\",user_input)\n",
    "    if user_input == 'q' or user_input == 'Q':\n",
    "        break\n",
    "    \n",
    "    assistant1 = ibm_watson.AssistantV1(\n",
    "        version='2019-02-28',\n",
    "        url=credentials_shiv['api_url'],\n",
    "        iam_apikey=credentials_shiv['api_key']\n",
    "    )\n",
    "    \n",
    "    response1 = assistant1.message(\n",
    "        workspace_id=credentials_shiv['workspace_id'],\n",
    "        input={\n",
    "            'text': user_input\n",
    "        }\n",
    "    ).get_result()    \n",
    "    \n",
    "    ## here's the other bot\n",
    "    \n",
    "    assistant2 = ibm_watson.AssistantV1(\n",
    "        version='2019-02-28',\n",
    "        url=credentials_moh['api_url'],\n",
    "        iam_apikey=credentials_moh['api_key']\n",
    "    )    \n",
    "\n",
    "    response2 = assistant2.message(\n",
    "        workspace_id=credentials_moh['workspace_id'],\n",
    "        input={\n",
    "            'text': user_input\n",
    "        }\n",
    "    ).get_result()  \n",
    "    \n",
    "#     print(response['entities'])\n",
    "\n",
    "#     print(response2)\n",
    "    \n",
    "    query1 = []\n",
    "    tags1 = []\n",
    "    for i in response1['entities']:\n",
    "        if i['entity'] == 'query':\n",
    "            query1.append(i['value'])\n",
    "\n",
    "        if i['entity'] == 'tags':\n",
    "            tags1.append(i['value'] + ':' +str(i['confidence']))\n",
    "\n",
    "    if len(tags) >= 1:\n",
    "        print('CHATBOT_shiv TAGS ARE :',tags1)\n",
    "    \n",
    "    query2 = []\n",
    "    tags2 = []\n",
    "    for i in response2['entities']:\n",
    "        if i['entity'] == 'query':\n",
    "            query2.append(i['value'])\n",
    "\n",
    "        if i['entity'] == 'Stack_Overflow_Tag':\n",
    "            tags2.append(i['value'] + ':' +str(i['confidence']))\n",
    "\n",
    "    if len(tags) >= 1:\n",
    "        print('CHATBOT_moh TAGS ARE :',tags2)\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = 0\n",
    "# for i in range(value_list_use):\n",
    "#     c += 1\n",
    "#     print('        {')\n",
    "#     print('          \"type\": \"synonyms\",')\n",
    "#     temp = i.split('\\n')\n",
    "#     if len(temp) == 2:\n",
    "#         print('          \"value\": \"' + temp[0] + ' ' + temp[1] + '\",')\n",
    "#     else:\n",
    "#         print('          \"value\": \"' + temp[0] + '\",')\n",
    "#     print('          \"synonyms\": []')\n",
    "#     print('        },')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"QueryResults.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4621"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df[[\"SourceTagName\",\"TargetTagName\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_power = pd.DataFrame(new_df.groupby(['TargetTagName'])['SourceTagName'].apply(list),columns=[\"SourceTagName\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pythonic', 'python-interpreter', 'python-shell', 'py']"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_power.loc['python']['SourceTagName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_list_1 = pd.read_csv(\"QueryResults(3).csv\")\n",
    "value_list_2 = pd.read_csv(\"QueryResults(5).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cd = pd.merge(value_list_1,value_list_2, how='outer', on = 'TagName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_use = df_cd['TagName']\n",
    "df_use = df_use.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_list_use = list(df_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55224"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(value_list_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('SO_tags_dict_2.txt', 'r') as f:\n",
    "#     value_list = json.load(f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source length is  4621\n",
      "target length is  2986\n"
     ]
    }
   ],
   "source": [
    "Source_name = pd.Series(new_df['SourceTagName'])\n",
    "Target_name = pd.Series(list(set(new_df['TargetTagName'])))\n",
    "Source_name = Source_name.dropna()\n",
    "Target_name = Target_name.dropna()\n",
    "print(\"source length is \",len(Source_name))\n",
    "print(\"target length is \",len(Target_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in Target_name:\n",
    "    if i in value_list_use:\n",
    "#         print(i)\n",
    "        value_list_use.remove(i)\n",
    "    else:\n",
    "        count+=1\n",
    "#         print(i)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3278\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in Source_name:\n",
    "    if i in value_list_use:\n",
    "#         print(i)\n",
    "        value_list_use.remove(i)\n",
    "    else:\n",
    "        count+=1\n",
    "#         print(i)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "for i in Target_name:\n",
    "    values.append({\"type\":\"synonyms\",\"value\":i,\"synonyms\":x_power.loc[i]['SourceTagName']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in value_list_use:\n",
    "    values.append({\"type\":\"synonyms\",\"value\":i,\"synonyms\":[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(values,\"values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "so = load_obj(\"values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53934"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('skill-Stackoverflow_query.json', 'r') as f:\n",
    "    skill_stack = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_stack['entities'].append({\"entity\":\"Stack_Overflow_Tag\",\"values\":values,\"fuzzy_match\":True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skill_stack['entities'][0]['fuzzy_match']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('skill-Stackoverflow_query_4.json', 'w') as file:\n",
    "     json.dump(skill_stack,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
